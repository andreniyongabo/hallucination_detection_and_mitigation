{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e576d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1b087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/private/home/dlicht/flores101_data/2021_09_human_eval_sample_building\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f928531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'How to load FLORES data without PANDAS errors.ipynb'\r\n",
      "'NLLB October FLORES sampling-batch.ipynb'\r\n",
      "'NLLB October FLORES sampling.ipynb'\r\n",
      "'NLLB build Cross-Lingual Calibration Set v1.ipynb'\r\n",
      "'daiquery-unique flores IDs-2021-09-08 11_00am.tsv'\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "025f8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_tsv = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41bbc25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG3_list = [\n",
    "#     'rus',\n",
    "#     'deu',\n",
    "#     'hau',\n",
    "#     'isl',\n",
    "#     'ces',\n",
    "#     'jpn',\n",
    "    'snd',\n",
    "    'azj',\n",
    "    'amh',\n",
    "    'zul',\n",
    "    'kat',\n",
    "    'urd',\n",
    "    'ara',\n",
    "    'hin',\n",
    "    'slv',\n",
    "    'swh',\n",
    "    'bul',\n",
    "    'bos',\n",
    "    'ron',\n",
    "    'por',\n",
    "\n",
    "#     'zho_simpl',\n",
    "#     'fra',\n",
    "#     'kor',\n",
    "]\n",
    "\n",
    "LANG2_list = [\n",
    "#     'ru',\n",
    "#     'de',\n",
    "#     'ha',\n",
    "#     'is',\n",
    "#     'cs',\n",
    "#     'ja',\n",
    "    'sd',\n",
    "    'az',\n",
    "    'am',\n",
    "    'zu',\n",
    "    'ka',\n",
    "    'ur',\n",
    "    'ar',\n",
    "    'hi',\n",
    "    'sl',\n",
    "    'sw',\n",
    "    'bg',\n",
    "    'bs',\n",
    "    'ro',\n",
    "    'pt',\n",
    "\n",
    "#     'zho',\n",
    "#     'fr',\n",
    "#     'ko',\n",
    "]\n",
    "\n",
    "assert len(LANG2_list) == len(LANG3_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d29b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vendor info here:  https://docs.google.com/spreadsheets/d/17zyMnZf2DOGzmte0qarBDIjyXAfrB24yoqwf5IknRc0/edit#gid=192447989\n",
    "vendor_dict = {\n",
    "    'rus':'Moravia',\n",
    "    'deu':'Moravia',\n",
    "    'hau':'Moravia',\n",
    "    'isl':'Moravia',\n",
    "    'ces':'Moravia',\n",
    "    'jpn':'Moravia',\n",
    "    'snd':'Moravia',\n",
    "    'azj':'Lionbridge',\n",
    "    'amh':'Moravia',\n",
    "    'zul':'Lionbridge',\n",
    "    'kat':'Moravia',\n",
    "    'urd':'Lionbridge',\n",
    "    'ara':'Moravia',\n",
    "    'hin':'Lionbridge',\n",
    "    'slv':'Moravia',\n",
    "    'swh':'Lionbridge',\n",
    "    'bul':'Moravia',\n",
    "    'bos':'Lionbridge',\n",
    "    'ron':'Moravia',\n",
    "    'por':'Lionbridge',\n",
    "    'asm':'Lionbridge',\n",
    "    'umb':'Moravia',\n",
    "    'tel':'Lionbridge',\n",
    "    'kea':'Moravia',\n",
    "    'ceb':'Moravia',\n",
    "}\n",
    "\n",
    "# \n",
    "# vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf0cca08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langauge code - 3 letter: snd     2 letter: sd    Vendor:  MORAVIA\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_snd.tsv\n",
      "writing file 20210913_MORAVIA_ADHOC_NLLB-HE_H2-R1_2021_snd.tsv\n",
      "Langauge code - 3 letter: azj     2 letter: az    Vendor:  LIONBRIDGE\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_azj.tsv\n",
      "writing file 20210913_LIONBRIDGE_ADHOC_NLLB-HE_H2-R1_2021_azj.tsv\n",
      "Langauge code - 3 letter: amh     2 letter: am    Vendor:  MORAVIA\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_amh.tsv\n",
      "writing file 20210913_MORAVIA_ADHOC_NLLB-HE_H2-R1_2021_amh.tsv\n",
      "Langauge code - 3 letter: zul     2 letter: zu    Vendor:  LIONBRIDGE\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_zul.tsv\n",
      "writing file 20210913_LIONBRIDGE_ADHOC_NLLB-HE_H2-R1_2021_zul.tsv\n",
      "Langauge code - 3 letter: kat     2 letter: ka    Vendor:  MORAVIA\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_kat.tsv\n",
      "writing file 20210913_MORAVIA_ADHOC_NLLB-HE_H2-R1_2021_kat.tsv\n",
      "Langauge code - 3 letter: urd     2 letter: ur    Vendor:  LIONBRIDGE\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_urd.tsv\n",
      "writing file 20210913_LIONBRIDGE_ADHOC_NLLB-HE_H2-R1_2021_urd.tsv\n",
      "Langauge code - 3 letter: ara     2 letter: ar    Vendor:  MORAVIA\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_ara.tsv\n",
      "writing file 20210913_MORAVIA_ADHOC_NLLB-HE_H2-R1_2021_ara.tsv\n",
      "Langauge code - 3 letter: hin     2 letter: hi    Vendor:  LIONBRIDGE\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_hin.tsv\n",
      "writing file 20210913_LIONBRIDGE_ADHOC_NLLB-HE_H2-R1_2021_hin.tsv\n",
      "Langauge code - 3 letter: slv     2 letter: sl    Vendor:  MORAVIA\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_slv.tsv\n",
      "writing file 20210913_MORAVIA_ADHOC_NLLB-HE_H2-R1_2021_slv.tsv\n",
      "Langauge code - 3 letter: swh     2 letter: sw    Vendor:  LIONBRIDGE\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_swh.tsv\n",
      "writing file 20210913_LIONBRIDGE_ADHOC_NLLB-HE_H2-R1_2021_swh.tsv\n",
      "Langauge code - 3 letter: bul     2 letter: bg    Vendor:  MORAVIA\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_bul.tsv\n",
      "writing file 20210913_MORAVIA_ADHOC_NLLB-HE_H2-R1_2021_bul.tsv\n",
      "Langauge code - 3 letter: bos     2 letter: bs    Vendor:  LIONBRIDGE\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_bos.tsv\n",
      "writing file 20210913_LIONBRIDGE_ADHOC_NLLB-HE_H2-R1_2021_bos.tsv\n",
      "Langauge code - 3 letter: ron     2 letter: ro    Vendor:  MORAVIA\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_ron.tsv\n",
      "writing file 20210913_MORAVIA_ADHOC_NLLB-HE_H2-R1_2021_ron.tsv\n",
      "Langauge code - 3 letter: por     2 letter: pt    Vendor:  LIONBRIDGE\n",
      "(630, 4)\n",
      "max items with same fbid= 1\n",
      "writing file 20210913_eval_NLLB_m2m100_por.tsv\n",
      "writing file 20210913_LIONBRIDGE_ADHOC_NLLB-HE_H2-R1_2021_por.tsv\n"
     ]
    }
   ],
   "source": [
    "for L_key in zip(LANG2_list,LANG3_list):\n",
    "    L2 = L_key[0]\n",
    "    L3 = L_key[1]\n",
    "    vendor = vendor_dict[L3].upper()\n",
    "    print(\"Langauge code - 3 letter:\",L3, \"    2 letter:\", L2, \"   Vendor: \",vendor)\n",
    "    \n",
    "    \n",
    "    # Import and built the FLORES Human Refence Set\n",
    "\n",
    "    filename_v1_IDs = \"/private/home/dlicht/flores101_data/2021_09_human_eval_sample_building/daiquery-unique flores IDs-2021-09-08 11_00am.tsv\"\n",
    "    df_ID_v1 = pd.read_csv(filename_v1_IDs, \n",
    "                              delimiter='\\t', header=0, names=['FLORES_ID','s_verif','count'], quoting=3)\n",
    "    df_ID_v1 = df_ID_v1[['FLORES_ID','s_verif']]\n",
    "    df_ID_v1\n",
    "\n",
    "    filename_eng = \"/private/home/dlicht/flores101_data/FLORES_devtest/eng.devtest\"\n",
    "    df_F101_eng = pd.read_csv(filename_eng, \n",
    "                              delimiter='\\t', header=None, names=['sentence1'], quoting=3)\n",
    "    df_F101_eng.index.rename('FLORES_ID', inplace=True) \n",
    "    df_F101_eng['sentence1_lang'] = 'eng'\n",
    "    # df_F101_eng['eng'] = df_F101_eng['eng'].str.replace('\\\"','').str.replace(\"\\'\",'')\n",
    "    df_F101_eng\n",
    "\n",
    "    df_F101 = pd.read_csv(f\"/private/home/dlicht/flores101_data/FLORES_devtest/{L3}.devtest\", \n",
    "                              delimiter='\\t', header=None, names=['sentence2'],  quoting=3)\n",
    "    df_F101['sentence2_lang'] = L3\n",
    "    df_F101.index.rename('FLORES_ID', inplace=True)\n",
    "    df_F101\n",
    "\n",
    "    df_F101_J = pd.merge(df_F101_eng, df_F101, how='outer', on='FLORES_ID')\n",
    "    df_F101_J\n",
    "\n",
    "    df_F101_J[df_F101_J['sentence1'].str.contains('British')]\n",
    "\n",
    "\n",
    "\n",
    "    HT_v1_ID_list = list(df_ID_v1.FLORES_ID)\n",
    "    df_F101_v1_eng   = df_F101_eng[df_F101_eng.index.isin(df_ID_v1.FLORES_ID)]\n",
    "    df_F101_v1       = df_F101[df_F101.index.isin(HT_v1_ID_list)]\n",
    "\n",
    "    df_F101_v1_J = df_F101_v1_eng.merge(df_F101_v1, how='outer', on='FLORES_ID')\n",
    "    df_F101_v1_J\n",
    "\n",
    "\n",
    "\n",
    "    prop_sample = 0.5**0.5   # 1/sqrt(2)\n",
    "    prop_sample\n",
    "\n",
    "    sample_size_HT = int(df_F101_J.shape[0] * prop_sample)\n",
    "    sample_size_HT\n",
    "\n",
    "    sample_size_v1 = df_F101_v1_J.shape[0]\n",
    "    sample_size_v1\n",
    "\n",
    "    sample_size_suppletmentary_HT = sample_size_HT - sample_size_v1\n",
    "    sample_size_suppletmentary_HT\n",
    "\n",
    "    df_F101_v1_pool       = df_F101_J[   df_F101_J.index.isin(HT_v1_ID_list) ]\n",
    "    df_F101_untapped_pool = df_F101_J[ ~ df_F101_J.index.isin(HT_v1_ID_list) ]\n",
    "    print(df_F101_v1_pool.shape)\n",
    "    df_F101_untapped_pool\n",
    "\n",
    "    df_HT_sup_sample = df_F101_untapped_pool.sample(n=sample_size_suppletmentary_HT, replace=False)\n",
    "    df_HT_sup_sample.sort_values('FLORES_ID')\n",
    "\n",
    "    df_HT_sample = pd.concat([df_F101_v1_pool, df_HT_sup_sample])\n",
    "    df_HT_sample\n",
    "\n",
    "    # verification step\n",
    "    # This will throw an error if the HT sample size is wrong\n",
    "    assert df_HT_sample.shape[0] == sample_size_HT\n",
    "\n",
    "    df_HT_sample_forward = df_HT_sample.copy()\n",
    "    df_HT_sample_forward['direction'] = 'forward'\n",
    "    df_HT_sample_forward\n",
    "\n",
    "    df_HT_sample_backward = df_HT_sample.copy()\n",
    "    df_HT_sample_backward['direction'] = 'backward'\n",
    "    df_HT_sample_backward.rename(columns={\n",
    "                                        \"sentence1\":\"sentence2\",\n",
    "                                        \"sentence2\":\"sentence1\",\n",
    "                                        \"sentence1_lang\":\"sentence2_lang\",\n",
    "                                        \"sentence2_lang\":\"sentence1_lang\",\n",
    "                                        }, inplace=True)\n",
    "    df_HT_sample_backward\n",
    "\n",
    "    df_HT_sample_paired = pd.concat([df_HT_sample_forward, df_HT_sample_backward]).sort_values(['FLORES_ID','sentence1_lang','sentence2_lang'])\n",
    "    # create a hash to be our new ID\n",
    "    # df_HT_sample_paired['fbid'] = df_HT_sample_paired.apply(lambda x: hash(tuple(x)),axis=1)\n",
    "    df_HT_sample_paired['model']  = 'human'\n",
    "    df_HT_sample_paired['dataset']='devtest'\n",
    "    df_HT_sample_paired = df_HT_sample_paired[['sentence1_lang', 'sentence2_lang','sentence1', 'sentence2', \n",
    "           'model','direction','dataset']].reset_index()\n",
    "    df_HT_sample_paired\n",
    "\n",
    "    # Import the Machine Translations\n",
    "\n",
    "    L2\n",
    "\n",
    "    ### XX into EN\n",
    "\n",
    "    filename_eng = f\"/private/home/dlicht/flores101_data/m2m100_generations_21_10_09/{L2}_en_beam4.hyp\"\n",
    "    df_MT_into_EN = pd.read_csv(filename_eng, \n",
    "                              delimiter='\\t', header=None, names=['sentence2'], quoting=3)\n",
    "    df_MT_into_EN.index.rename('FLORES_ID', inplace=True) \n",
    "    df_MT_into_EN['sentence2_lang'] = 'eng'\n",
    "    df_MT_into_EN\n",
    "\n",
    "    df_F101_S = df_F101.rename(columns={'sentence2':'sentence1','sentence2_lang':'sentence1_lang'})\n",
    "    df_F101_S\n",
    "\n",
    "    df_MT_sample_backward = pd.merge(df_F101_S, df_MT_into_EN, how='right', on='FLORES_ID')\n",
    "    df_MT_sample_backward['direction'] = 'backward'\n",
    "    df_MT_sample_backward\n",
    "\n",
    "    ### EN into XX\n",
    "\n",
    "    filename = f\"/private/home/dlicht/flores101_data/m2m100_generations_21_10_09/en_{L2}_beam4.hyp\"\n",
    "    df_MT_from_EN = pd.read_csv(filename, \n",
    "                              delimiter='\\t', header=None, names=['sentence2'], quoting=3)\n",
    "    df_MT_from_EN.index.rename('FLORES_ID', inplace=True) \n",
    "    df_MT_from_EN['sentence2_lang'] = L3\n",
    "    df_MT_from_EN\n",
    "\n",
    "    df_F101_eng\n",
    "\n",
    "    df_MT_sample_forward = pd.merge(df_F101_eng, df_MT_from_EN, how='right', on='FLORES_ID')\n",
    "    df_MT_sample_forward['direction'] = 'forward'\n",
    "    df_MT_sample_forward\n",
    "\n",
    "    df_MT_sample_paired = pd.concat([df_MT_sample_forward, df_MT_sample_backward]).sort_values(['FLORES_ID','sentence1_lang','sentence2_lang'])\n",
    "    # create a hash to be our new ID\n",
    "    # df_HT_sample_paired['fbid'] = df_HT_sample_paired.apply(lambda x: hash(tuple(x)),axis=1)\n",
    "    df_MT_sample_paired['model']  = 'm2m100'\n",
    "    df_MT_sample_paired['dataset']='devtest'\n",
    "    df_MT_sample_paired = df_MT_sample_paired[['sentence1_lang', 'sentence2_lang','sentence1', 'sentence2', \n",
    "           'model','direction','dataset']].reset_index()\n",
    "    df_MT_sample_paired\n",
    "\n",
    "\n",
    "\n",
    "    # Import the Cross-Lingual Calibration Set\n",
    "\n",
    "    filename = \"Cross_Lingual_Calibration_Set_v1.tsv\"\n",
    "    write_dir = \"/private/home/dlicht/fairseq-py/examples/nllb/human_eval_sample_generation/\"\n",
    "\n",
    "    df_CLC = pd.read_csv(write_dir+filename, delimiter='\\t')\n",
    "    df_CLC.drop(columns=['model','backtranslation_lang',\n",
    "           'sampled_quality_score', 'quality_source'], inplace=True)\n",
    "    df_CLC['model'] = 'Cross_Lingual_Calibration'\n",
    "    df_CLC['direction'] = 'backward_calibration'\n",
    "    df_CLC\n",
    "\n",
    "    # Combine the 3 sets together\n",
    "\n",
    "    df_sample_union = pd.concat([df_HT_sample_paired, df_MT_sample_paired, df_CLC])\n",
    "    df_sample_union\n",
    "\n",
    "    df_HT_sample_paired.groupby('model').count()\n",
    "\n",
    "    df_sample_union['fbid'] = pd.util.hash_pandas_object(df_sample_union)  # hash_pandas_object yields only positive hashesdf_HT_sample_paired = \n",
    "    df_sample_union.tail(2)\n",
    "\n",
    "    # throw an error if there are any duplicat fbid hashes\n",
    "    print('max items with same fbid=',df_sample_union.groupby('fbid').count().max().max())\n",
    "    assert df_sample_union.groupby('fbid').count().max().max() == 1\n",
    "\n",
    "\n",
    "\n",
    "    # Randomize the Sample Order\n",
    "\n",
    "    # randomize the sample\n",
    "    df_final_sample = df_sample_union.sample(frac=1, replace=False)\n",
    "    df_final_sample.set_index('fbid', inplace=True)\n",
    "    df_final_sample\n",
    "\n",
    "    # Write to Disk\n",
    "\n",
    "    write_dir\n",
    "\n",
    "\n",
    "\n",
    "    #write internal version\n",
    "    filename = f\"20210913_eval_NLLB_m2m100_{L3}.tsv\"\n",
    "    write_dir = '/private/home/dlicht/fairseq-py/examples/nllb/human_eval_sample_generation/master_copy/'\n",
    "\n",
    "    if write_tsv:\n",
    "        print(f\"writing file {filename}\")\n",
    "        df_final_sample.to_csv(write_dir+filename, sep='\\t',)\n",
    "\n",
    "    df_final_sample.columns\n",
    "\n",
    "    df_vendor_subset = df_final_sample[['sentence1_lang', 'sentence2_lang', 'sentence1',\n",
    "           'sentence2']]\n",
    "    df_vendor_subset\n",
    "\n",
    "    # write vendor version\n",
    "    filename = f\"20210913_{vendor}_ADHOC_NLLB-HE_H2-R1_2021_{L3}.tsv\"\n",
    "    write_dir = '/private/home/dlicht/fairseq-py/examples/nllb/human_eval_sample_generation/vendor_copy/'\n",
    "\n",
    "    if write_tsv:\n",
    "        print(f\"writing file {filename}\")\n",
    "        df_vendor_subset.to_csv(write_dir+filename, sep='\\t',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f007bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
