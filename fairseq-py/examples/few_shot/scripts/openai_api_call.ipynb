{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "# Load your API key from an environment variable or secret management service\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['davinci-instruct-beta-v3',\n",
       " 'curie-instruct-beta-v2',\n",
       " 'babbage-instruct-beta',\n",
       " 'ada-instruct-beta',\n",
       " 'text-davinci-001',\n",
       " 'text-curie-001',\n",
       " 'text-babbage-001',\n",
       " 'text-ada-001',\n",
       " 'davinci-instruct-beta',\n",
       " 'curie-instruct-beta',\n",
       " 'davinci',\n",
       " 'curie',\n",
       " 'babbage',\n",
       " 'ada']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine_names = \"\"\"\n",
    "davinci-instruct-beta-v3\n",
    "curie-instruct-beta-v2\n",
    "babbage-instruct-beta\n",
    "ada-instruct-beta\n",
    "text-davinci-001\n",
    "text-curie-001\n",
    "text-babbage-001\n",
    "text-ada-001\n",
    "davinci-instruct-beta\n",
    "curie-instruct-beta\n",
    "davinci\n",
    "curie\n",
    "babbage\n",
    "ada\n",
    "\"\"\"\n",
    "engine_names = [y for y in [x.strip() for x in engine_names.split(\"\\n\")] if len(y) > 0]\n",
    "engine_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling davinci-instruct-beta-v3\n",
      "calling curie-instruct-beta-v2\n",
      "calling babbage-instruct-beta\n",
      "calling ada-instruct-beta\n",
      "calling text-davinci-001\n",
      "calling text-curie-001\n",
      "calling text-babbage-001\n",
      "calling text-ada-001\n",
      "calling davinci-instruct-beta\n",
      "calling curie-instruct-beta\n",
      "calling davinci\n",
      "calling curie\n",
      "calling babbage\n",
      "calling ada\n"
     ]
    }
   ],
   "source": [
    "response_infos = []\n",
    "for engine_name in engine_names:\n",
    "    print(f\"calling {engine_name}\")\n",
    "    response_info = {\"engine_name\": engine_name}\n",
    "    response = openai.Completion.create(engine=engine_name, \n",
    "                                        prompt=\"Test api\", max_tokens=0, logprobs=1, echo=True)\n",
    "    response_info[\"model\"] = response[\"model\"]\n",
    "    response_info[\"response\"] = response\n",
    "    response_infos.append(response_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>engine_name</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ada:2020-05-03</th>\n",
       "      <td>ada</td>\n",
       "      <td>{'id': 'cmpl-4XG6pqmi9fArwLH8FHSkxDix3C1Mf', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>babbage:2020-05-03</th>\n",
       "      <td>babbage</td>\n",
       "      <td>{'id': 'cmpl-4XG6pa5aploLby4BG6K8WNQFRNeVt', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curie:2020-05-03</th>\n",
       "      <td>curie</td>\n",
       "      <td>{'id': 'cmpl-4XG6nDpsG75luBvxmZPoub1HkeHoo', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>davinci:2020-05-03</th>\n",
       "      <td>davinci</td>\n",
       "      <td>{'id': 'cmpl-4XG6nD4O7sNV6fjvJP3ctgpxuQA3P', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if-curie-v2</th>\n",
       "      <td>curie-instruct-beta</td>\n",
       "      <td>{'id': 'cmpl-4XG6mLSXgdzIl83WarXza4WZwJHYK', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>if-davinci-v2</th>\n",
       "      <td>davinci-instruct-beta</td>\n",
       "      <td>{'id': 'cmpl-4XG6mKvuP99aX7GdlHJXbceK0NKsE', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-ada:001</th>\n",
       "      <td>ada-instruct-beta</td>\n",
       "      <td>{'id': 'cmpl-4XG6hjayAH1E8SeJijqp75HkDrMxz', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-ada:001</th>\n",
       "      <td>text-ada-001</td>\n",
       "      <td>{'id': 'cmpl-4XG6mHB4vlVxzAua9gBrirntIpEGN', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-babbage:001</th>\n",
       "      <td>babbage-instruct-beta</td>\n",
       "      <td>{'id': 'cmpl-4XG6hPMokqOUgscCKrLjN8CORPZUo', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-babbage:001</th>\n",
       "      <td>text-babbage-001</td>\n",
       "      <td>{'id': 'cmpl-4XG6kzzstakc8k9BP3eNzksS99oBX', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-curie:001</th>\n",
       "      <td>curie-instruct-beta-v2</td>\n",
       "      <td>{'id': 'cmpl-4XG6hbSuWdI10DRx54UKizg8Byoc4', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-curie:001</th>\n",
       "      <td>text-curie-001</td>\n",
       "      <td>{'id': 'cmpl-4XG6jvIVwq3ZKzXP0esLIfu3EqRSM', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci:001</th>\n",
       "      <td>davinci-instruct-beta-v3</td>\n",
       "      <td>{'id': 'cmpl-4XG6hv4HSGXQx4tsv09P67pM1ZXQM', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-davinci:001</th>\n",
       "      <td>text-davinci-001</td>\n",
       "      <td>{'id': 'cmpl-4XG6i1WsUAA8MwqPxO3RS9iZmWcI6', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 engine_name  \\\n",
       "model                                          \n",
       "ada:2020-05-03                           ada   \n",
       "babbage:2020-05-03                   babbage   \n",
       "curie:2020-05-03                       curie   \n",
       "davinci:2020-05-03                   davinci   \n",
       "if-curie-v2              curie-instruct-beta   \n",
       "if-davinci-v2          davinci-instruct-beta   \n",
       "text-ada:001               ada-instruct-beta   \n",
       "text-ada:001                    text-ada-001   \n",
       "text-babbage:001       babbage-instruct-beta   \n",
       "text-babbage:001            text-babbage-001   \n",
       "text-curie:001        curie-instruct-beta-v2   \n",
       "text-curie:001                text-curie-001   \n",
       "text-davinci:001    davinci-instruct-beta-v3   \n",
       "text-davinci:001            text-davinci-001   \n",
       "\n",
       "                                                             response  \n",
       "model                                                                  \n",
       "ada:2020-05-03      {'id': 'cmpl-4XG6pqmi9fArwLH8FHSkxDix3C1Mf', '...  \n",
       "babbage:2020-05-03  {'id': 'cmpl-4XG6pa5aploLby4BG6K8WNQFRNeVt', '...  \n",
       "curie:2020-05-03    {'id': 'cmpl-4XG6nDpsG75luBvxmZPoub1HkeHoo', '...  \n",
       "davinci:2020-05-03  {'id': 'cmpl-4XG6nD4O7sNV6fjvJP3ctgpxuQA3P', '...  \n",
       "if-curie-v2         {'id': 'cmpl-4XG6mLSXgdzIl83WarXza4WZwJHYK', '...  \n",
       "if-davinci-v2       {'id': 'cmpl-4XG6mKvuP99aX7GdlHJXbceK0NKsE', '...  \n",
       "text-ada:001        {'id': 'cmpl-4XG6hjayAH1E8SeJijqp75HkDrMxz', '...  \n",
       "text-ada:001        {'id': 'cmpl-4XG6mHB4vlVxzAua9gBrirntIpEGN', '...  \n",
       "text-babbage:001    {'id': 'cmpl-4XG6hPMokqOUgscCKrLjN8CORPZUo', '...  \n",
       "text-babbage:001    {'id': 'cmpl-4XG6kzzstakc8k9BP3eNzksS99oBX', '...  \n",
       "text-curie:001      {'id': 'cmpl-4XG6hbSuWdI10DRx54UKizg8Byoc4', '...  \n",
       "text-curie:001      {'id': 'cmpl-4XG6jvIVwq3ZKzXP0esLIfu3EqRSM', '...  \n",
       "text-davinci:001    {'id': 'cmpl-4XG6hv4HSGXQx4tsv09P67pM1ZXQM', '...  \n",
       "text-davinci:001    {'id': 'cmpl-4XG6i1WsUAA8MwqPxO3RS9iZmWcI6', '...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(response_infos)\n",
    "df = df.sort_values(by='model')\n",
    "df.set_index(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0,\n",
      "          4\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          null,\n",
      "          -10.3049135\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"Test\",\n",
      "          \" api\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          null,\n",
      "          {\n",
      "            \"_\": -2.4764898\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"Test api\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1643832095,\n",
      "  \"id\": \"cmpl-4XG6pqmi9fArwLH8FHSkxDix3C1Mf\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time\n",
    "def call_gpt3(prompt, engine, max_tokens_to_generate=0, temperature=0, logprobs_per_token=1, echo=True, stop_token=None, n=None, max_tries=10):\n",
    "    response = None\n",
    "    success = False\n",
    "    \n",
    "    tries_cnt = 0\n",
    "    while not success:\n",
    "        try:\n",
    "            if tries_cnt >= max_tries:\n",
    "                print(f\"Max tries {max_tries} reached!\")\n",
    "                break\n",
    "                \n",
    "            tries_cnt += 1\n",
    "            response = openai.Completion.create(engine=engine, \n",
    "                                                prompt=prompt,\n",
    "                                                max_tokens=max_tokens_to_generate,\n",
    "                                                temperature=temperature,\n",
    "                                                logprobs=logprobs_per_token,\n",
    "                                                echo=echo,\n",
    "                                                stop=stop_token,\n",
    "                                                n=n)\n",
    "            \n",
    "            \n",
    "            success = True\n",
    "        except openai.error.InvalidRequestError as error: \n",
    "            print(f\"InvalidRequestError:{error}\\nPrompt sent:\\n{prompt}\\n\")\n",
    "            raise error\n",
    "        except Exception as error:\n",
    "            print(f\"API error:{error}\")\n",
    "            time.sleep(1)\n",
    "            \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"length\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": {\n",
      "        \"text_offset\": [\n",
      "          0,\n",
      "          1,\n",
      "          2\n",
      "        ],\n",
      "        \"token_logprobs\": [\n",
      "          null,\n",
      "          -7.3669047,\n",
      "          -6.4439845\n",
      "        ],\n",
      "        \"tokens\": [\n",
      "          \"8\",\n",
      "          \"Y\",\n",
      "          \"o\"\n",
      "        ],\n",
      "        \"top_logprobs\": [\n",
      "          null,\n",
      "          {\n",
      "            \".\": -2.4928317\n",
      "          },\n",
      "          {\n",
      "            \"5\": -3.7978659\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      \"text\": \"8Yo\"\n",
      "    }\n",
      "  ],\n",
      "  \"created\": 1622658654,\n",
      "  \"id\": \"cmpl-36Pvil7L3NGVRLYqni1p90VyUF9oB\",\n",
      "  \"model\": \"ada:2020-05-03\",\n",
      "  \"object\": \"text_completion\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = call_gpt3([23,56,78], \"ada\", max_tries=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['8', 'Y', 'o'], 'score': None, 'attention': None, 'alignment': None, 'positional_scores': tensor([-6.9054, -7.3669, -6.4440]), 'gpt3_response': {'id': 'cmpl-36Pvil7L3NGVRLYqni1p90VyUF9oB', 'object': 'text_completion', 'created': 1622658654, 'model': 'ada:2020-05-03', 'choices': [{'text': '8Yo', 'index': 0, 'logprobs': {'tokens': ['8', 'Y', 'o'], 'token_logprobs': [-6.9054446, -7.3669047, -6.4439845], 'top_logprobs': [None, {'.': -2.4928317}, {'5': -3.7978659}], 'text_offset': [0, 1, 2]}, 'finish_reason': 'length'}]}}\n"
     ]
    }
   ],
   "source": [
    "#print(result)\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def openai_result_to_json(result):\n",
    "    try:\n",
    "        return json.loads(json.dumps(result))\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def openai_result_to_fairseq_result(result):\n",
    "    result_tokens = result[\"choices\"][0][\"logprobs\"][\"tokens\"]\n",
    "    result_logprobs = result[\"choices\"][0][\"logprobs\"][\"token_logprobs\"]\n",
    "    if result_logprobs[0] is None and len(result_logprobs)>1:\n",
    "        result_logprobs[0] = np.mean(result_logprobs[1:])\n",
    "        \n",
    "    fairseq_result = {'tokens': result_tokens,\n",
    "     'score': None, \n",
    "     'attention': None, \n",
    "     'alignment': None, \n",
    "     'positional_scores': torch.tensor(result_logprobs, dtype=torch.float32)}\n",
    "    fairseq_result[\"gpt3_response\"] = openai_result_to_json(result)\n",
    "    \n",
    "    return fairseq_result\n",
    "\n",
    "print(openai_result_to_fairseq_result(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= [{'tokens': []\n",
    "     'score': 0.6, \n",
    "     'attention': None, \n",
    "     'alignment': None, \n",
    "     'positional_scores': []}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_prefix_and_suffix_lengths(tokens_list):\n",
    "    tokens_list = [np.array(x) for x in tokens_list]\n",
    "    n = min([len(tokens) for tokens in tokens_list])\n",
    "    # Compute common prefix length\n",
    "    prefix_len = n\n",
    "    first = tokens_list[0][:n]\n",
    "    for tokens in tokens_list[1:]:\n",
    "        neq_inds = (first != tokens[:n]).nonzero()[0]\n",
    "        if len(neq_inds) > 0:\n",
    "            prefix_len = min(prefix_len, neq_inds[0].item())\n",
    "\n",
    "    # Compute common suffix length\n",
    "    suffix_len = n\n",
    "    first = tokens_list[0][-n:]\n",
    "    for tokens in tokens_list[1:]:\n",
    "        neq_inds = (first != tokens[-n:]).nonzero()[0]\n",
    "        if len(neq_inds) > 0:\n",
    "            suffix_len = min(suffix_len, n - 1 - neq_inds[-1].item())\n",
    "\n",
    "    return prefix_len, suffix_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_common_prefix_and_suffix_lengths([[\"a\", \"b\", \"c\"], [\"a\", \"b\", \"d\", \"e\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq-20210102",
   "language": "python",
   "name": "fairseq-20210102"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
