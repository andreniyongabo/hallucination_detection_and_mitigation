{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging task prompts\n",
    "This notebook can be used to debug task prompts.\n",
    "To do that we want to modify the template (the example is with copa) by creating a new template class (e.g. COPAGPT3Template) at examples/few_shot/templates.py usually by inheriting the default class (COPATemplate). We do not want to update the template in place since we might have experiments run with the existing templates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os    \n",
    "import glob\n",
    "from pathlib import Path\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update the parameters below to correspond to our new defined class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"125M_gpt3_setting\"\n",
    "#model_name = \"dense_lang16\"\n",
    "predictor_name=\"clmprompting\"\n",
    "# model_name = \"openai_ada\"\n",
    "# predictor_name=\"CLMPromptingOpenaiApi\"\n",
    "\n",
    "scoring = \"sum\"\n",
    "\n",
    "#debug_task=\"cb\"\n",
    "#template_name=\"cb_gpt3_reproduce\"\n",
    "\n",
    "# XNLI\n",
    "debug_task=\"xnli\"\n",
    "template_name=\"generativenli\"\n",
    "#template_name=\"xnli_generativenli_sentence_mt\"\n",
    "template_name=\"xnli_generativenli_ht\"\n",
    "calibrator_name = \"average_option\"\n",
    "calibration_options = [\"sentence1::\"]\n",
    "\n",
    "\n",
    "# #PAWSX\n",
    "# debug_task=\"pawsx\"\n",
    "# template_name=\"pawsx_mt\"\n",
    "# calibrator_name = \"average_option\"\n",
    "# calibration_options = [\"sentence1::\"]\n",
    "\n",
    "train_set=\"dev\"\n",
    "train_lang=\"es\"\n",
    "languages=[\"es\"]\n",
    "\n",
    "current_user = os.getenv(\"USER\")\n",
    "time_stamp = datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "results_dir = f\"/checkpoint/{current_user}/few_shot/debug_prompts/pred_{debug_task}_{template_name}_{model_name}_{time_stamp}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment \n",
    "We want to run experiment with some small parameters since the prompt generation for few-shot learning usually depends on some logic in the predictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name=125M_gpt3_setting\n",
      "model_pretraining_valid_info={\"epoch\": 3, \"valid_loss\": \"3.926\", \"valid_ppl\": \"15.2\", \"valid_wps\": \"857595\", \"valid_wpb\": \"216347\", \"valid_bsz\": \"105.7\", \"valid_num_updates\": \"572204\", \"valid_best_loss\": \"3.926\", \"time_stamp\": \"2020-11-04 04:05:02\", \"log_file\": \"/large_experiments/xlmg/models/dense/125M/few_shot.roberta+cc100.os.bm_none.tps2048.transformer_lm_gpt.share.adam.b2_0.98.eps1e-08.cl0.0.lr0.005.wu715.dr0.1.atdr0.1.wd0.01.ms4.uf2.mu572204.s1.ngpu32/train.log\"}\n",
      "distributed_training.distributed_port=-1\n",
      "Loaded model\n",
      "model_loading_time=4.5 seconds\n",
      "model_loading_time_cuda=6.0 seconds\n",
      "Changing max_positions from 2048 to 1024\n",
      "task=xnli\n",
      "eval_set=dev\n",
      "eval language=es\n",
      "train_set=test\n",
      "train_lang=es\n",
      "template=xnli_generativenli_ht\n",
      "calibration_options=['sentence1::']\n",
      "nb_few_shot_samples=0\n",
      "* using calibrator: AverageOptionCalibrator()\n",
      "expected_max_tgt_len=50, max_positions=1024\n",
      "Average number of train samples: 0.00\n",
      "Predicting 3 samples with 9 prompts..\n",
      "Before running model, bs=1, max_tgt_len=50 mem=0.24GB\n",
      "Generating calibration samples...\n",
      "Predicting calibration samples...\n",
      "expected_max_tgt_len=29, max_positions=1024\n",
      "Average number of train samples: 0.00\n",
      "Predicting 3 samples with 9 prompts..\n",
      "Before running model, bs=1, max_tgt_len=30 mem=0.27GB\n",
      "Predictions dumped to /checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_generativenli_ht_125M_gpt3_setting_2021-09-17-17-30-00/task.xnli_tmp.xnli_generativenli_ht_train.test.es_val.None.es_eval.dev.es_calib.average_option_fs0_seed0_predictions.jsonl\n",
      "results={'model_name': '125M_gpt3_setting', 'task': 'xnli', 'language': 'es', 'template': 'xnli_generativenli_ht', 'nb_few_shot_samples': 0, 'calibration_options': ['sentence1::'], 'calibrator_name': 'average_option', 'train_set': 'test', 'valid_set': None, 'eval_set': 'dev', 'train_lang': 'es', 'valid_lang': 'es', 'ppl_common_prefix': {'scores': [100.00114440917969], 'mean': 100.00114440917969, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_selected_candidate': {'scores': [19.288203875223797], 'mean': 19.288203875223797, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_selected_candidate': {'scores': [43.29908053080241], 'mean': 43.29908053080241, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt__entailment': {'scores': [47.134543100992836], 'mean': 47.134543100992836, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt__contradiction': {'scores': [49.30385208129883], 'mean': 49.30385208129883, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt__neutral': {'scores': [40.39980506896973], 'mean': 40.39980506896973, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt': {'scores': [45.61273341708713], 'mean': 45.61273341708713, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates': {'scores': [21.834955745273167], 'mean': 21.834955745273167, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_common_prefix': {'scores': [nan], 'mean': nan, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_selected_candidate': {'scores': [38.85954729715983], 'mean': 38.85954729715983, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_full_selected_candidate': {'scores': [38.85954729715983], 'mean': 38.85954729715983, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_candidates_full_prompt__entailment': {'scores': [45.97881253560384], 'mean': 45.97881253560384, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_candidates_full_prompt__contradiction': {'scores': [91.17705917358398], 'mean': 91.17705917358398, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_candidates_full_prompt__neutral': {'scores': [37.30179532368978], 'mean': 37.30179532368978, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_candidates_full_prompt': {'scores': [58.15255567762587], 'mean': 58.15255567762587, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_calib_candidates': {'scores': [58.15255567762587], 'mean': 58.15255567762587, 'std': 0.0, 'mean_confidence_interval': nan}, 'accuracy': {'scores': [33.333333333333336], 'mean': 33.333333333333336, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_correct_gold': {'scores': [20.277676582336426], 'mean': 20.277676582336426, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_incorrect_gold': {'scores': [22.613595326741535], 'mean': 22.613595326741535, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_incorrect_std_gold': {'scores': [1.828823725382487], 'mean': 1.828823725382487, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_incorrect_min_gold': {'scores': [20.78477160135905], 'mean': 20.78477160135905, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_correct_lt_incorrect_gold': {'scores': [33.333333333333336], 'mean': 33.333333333333336, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_correct_gold': {'scores': [45.00508117675781], 'mean': 45.00508117675781, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_incorrect_gold': {'scores': [45.91655953725179], 'mean': 45.91655953725179, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_incorrect_std_gold': {'scores': [2.793416659037272], 'mean': 2.793416659037272, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_incorrect_min_gold': {'scores': [43.123142878214516], 'mean': 43.123142878214516, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_correct_lt_incorrect_gold': {'scores': [33.333333333333336], 'mean': 33.333333333333336, 'std': 0.0, 'mean_confidence_interval': nan}, 'execution_time': {'scores': [3.2691703736782074], 'mean': 3.2691703736782074, 'std': 0.0, 'mean_confidence_interval': nan}}\n",
      "\n",
      "ppl_selected_candidate                          = 19.2882\n",
      "ppl_full_selected_candidate                     = 43.2991\n",
      "ppl_full_incorrect_std_gold                     = 2.7934\n",
      "ppl_full_incorrect_min_gold                     = 43.1231\n",
      "ppl_full_incorrect_gold                         = 45.9166\n",
      "ppl_full_correct_lt_incorrect_gold              = 33.3333\n",
      "ppl_full_correct_gold                           = 45.0051\n",
      "ppl_common_prefix                               = 100.0011\n",
      "ppl_candidates_full_prompt__neutral             = 40.3998\n",
      "ppl_candidates_full_prompt__entailment          = 47.1345\n",
      "ppl_candidates_full_prompt__contradiction       = 49.3039\n",
      "ppl_candidates_full_prompt                      = 45.6127\n",
      "ppl_candidates                                  = 21.835\n",
      "ppl_calib_selected_candidate                    = 38.8595\n",
      "ppl_calib_full_selected_candidate               = 38.8595\n",
      "ppl_calib_common_prefix                         = nan\n",
      "ppl_calib_candidates_full_prompt__neutral       = 37.3018\n",
      "ppl_calib_candidates_full_prompt__entailment    = 45.9788\n",
      "ppl_calib_candidates_full_prompt__contradiction = 91.1771\n",
      "ppl_calib_candidates_full_prompt                = 58.1526\n",
      "ppl_calib_candidates                            = 58.1526\n",
      "ppl_answer_incorrect_std_gold                   = 1.8288\n",
      "ppl_answer_incorrect_min_gold                   = 20.7848\n",
      "ppl_answer_incorrect_gold                       = 22.6136\n",
      "ppl_answer_correct_lt_incorrect_gold            = 33.3333\n",
      "ppl_answer_correct_gold                         = 20.2777\n",
      "execution_time                                  = 3.2692\n",
      "accuracy                                        = 33.3333\n"
     ]
    }
   ],
   "source": [
    "from examples.few_shot.gpt3_eval import run_evaluations_from_model_name\n",
    "\n",
    "args = {\n",
    "    \"model_name\": model_name,\n",
    "    \"tasks\": [debug_task],\n",
    "    f\"{debug_task}_template\": template_name,\n",
    "    \"nb_few_shot_samples_values\": [0],\n",
    "    \"n_eval_samples\": 3, # set 1 for debug\n",
    "    \"num_trials\": 1,\n",
    "    \"max_positions\": 1024,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"scoring\": scoring,\n",
    "    \"calibrator_name\": calibrator_name,\n",
    "    f\"{debug_task}_calibration_options\": calibration_options,\n",
    "    f\"{debug_task}_languages\": languages,\n",
    "    #\"train_set\": train_set,\n",
    "    \"{debug_task}_train_lang\": train_lang,\n",
    "    \"predictions_dump_dir\": results_dir,\n",
    "    \"results_dir\": results_dir,\n",
    "    \"add_prompt_to_meta\": True,\n",
    "    \"add_positional_scores_to_meta\": True, \n",
    "    \"add_prompt_tokens_to_meta\": True,\n",
    "    \"add_calib_meta\": True,\n",
    "    \"train_sep\": \"\\n\\n\",\n",
    "    #field_sep=\"\\n\",\n",
    "    \"uniform_sampling\": False,\n",
    "    \"predictor_name\": predictor_name, #\"CLMPromptingOpenaiApi\",\n",
    "}\n",
    "results = run_evaluations_from_model_name(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read predictions file\n",
    "We will read the predictions file which has the generated prompt, token ids and scores tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_name': '125M_gpt3_setting',\n",
       "  'task': 'xnli',\n",
       "  'language': 'es',\n",
       "  'template': 'xnli_generativenli_ht',\n",
       "  'nb_few_shot_samples': 0,\n",
       "  'calibration_options': ['sentence1::'],\n",
       "  'calibrator_name': 'average_option',\n",
       "  'train_set': 'test',\n",
       "  'valid_set': None,\n",
       "  'eval_set': 'dev',\n",
       "  'train_lang': 'es',\n",
       "  'valid_lang': 'es',\n",
       "  'ppl_common_prefix': {'scores': [100.00114440917969],\n",
       "   'mean': 100.00114440917969,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_selected_candidate': {'scores': [19.288203875223797],\n",
       "   'mean': 19.288203875223797,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_selected_candidate': {'scores': [43.29908053080241],\n",
       "   'mean': 43.29908053080241,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt__entailment': {'scores': [47.134543100992836],\n",
       "   'mean': 47.134543100992836,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt__contradiction': {'scores': [49.30385208129883],\n",
       "   'mean': 49.30385208129883,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt__neutral': {'scores': [40.39980506896973],\n",
       "   'mean': 40.39980506896973,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt': {'scores': [45.61273341708713],\n",
       "   'mean': 45.61273341708713,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates': {'scores': [21.834955745273167],\n",
       "   'mean': 21.834955745273167,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_common_prefix': {'scores': [nan],\n",
       "   'mean': nan,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_selected_candidate': {'scores': [38.85954729715983],\n",
       "   'mean': 38.85954729715983,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_full_selected_candidate': {'scores': [38.85954729715983],\n",
       "   'mean': 38.85954729715983,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_candidates_full_prompt__entailment': {'scores': [45.97881253560384],\n",
       "   'mean': 45.97881253560384,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_candidates_full_prompt__contradiction': {'scores': [91.17705917358398],\n",
       "   'mean': 91.17705917358398,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_candidates_full_prompt__neutral': {'scores': [37.30179532368978],\n",
       "   'mean': 37.30179532368978,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_candidates_full_prompt': {'scores': [58.15255567762587],\n",
       "   'mean': 58.15255567762587,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_calib_candidates': {'scores': [58.15255567762587],\n",
       "   'mean': 58.15255567762587,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'accuracy': {'scores': [33.333333333333336],\n",
       "   'mean': 33.333333333333336,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_correct_gold': {'scores': [20.277676582336426],\n",
       "   'mean': 20.277676582336426,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_incorrect_gold': {'scores': [22.613595326741535],\n",
       "   'mean': 22.613595326741535,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_incorrect_std_gold': {'scores': [1.828823725382487],\n",
       "   'mean': 1.828823725382487,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_incorrect_min_gold': {'scores': [20.78477160135905],\n",
       "   'mean': 20.78477160135905,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_correct_lt_incorrect_gold': {'scores': [33.333333333333336],\n",
       "   'mean': 33.333333333333336,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_correct_gold': {'scores': [45.00508117675781],\n",
       "   'mean': 45.00508117675781,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_incorrect_gold': {'scores': [45.91655953725179],\n",
       "   'mean': 45.91655953725179,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_incorrect_std_gold': {'scores': [2.793416659037272],\n",
       "   'mean': 2.793416659037272,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_incorrect_min_gold': {'scores': [43.123142878214516],\n",
       "   'mean': 43.123142878214516,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_correct_lt_incorrect_gold': {'scores': [33.333333333333336],\n",
       "   'mean': 33.333333333333336,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'execution_time': {'scores': [3.2691703736782074],\n",
       "   'mean': 3.2691703736782074,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'run_params': {'xnli_template': 'xnli_generativenli_ht',\n",
       "   'n_eval_samples': 3,\n",
       "   'max_positions': 1024,\n",
       "   'max_tokens': 1024,\n",
       "   'scoring': 'sum',\n",
       "   'calibrator_name': 'average_option',\n",
       "   'xnli_calibration_options': ['sentence1::'],\n",
       "   'xnli_languages': ['es'],\n",
       "   '{debug_task}_train_lang': 'es',\n",
       "   'predictions_dump_dir': '/checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_generativenli_ht_125M_gpt3_setting_2021-09-17-17-30-00',\n",
       "   'results_dir': '/checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_generativenli_ht_125M_gpt3_setting_2021-09-17-17-30-00',\n",
       "   'add_prompt_to_meta': True,\n",
       "   'add_positional_scores_to_meta': True,\n",
       "   'add_prompt_tokens_to_meta': True,\n",
       "   'add_calib_meta': True,\n",
       "   'train_sep': '\\n\\n',\n",
       "   'uniform_sampling': False,\n",
       "   'predictor_name': 'clmprompting',\n",
       "   'model_name_display': '125M_gpt3_setting'}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_generativenli_ht_125M_gpt3_setting_2021-09-17-17-30-00/task.xnli_tmp.xnli_generativenli_ht_train.test.es_val.None.es_eval.dev.es_calib.average_option_fs0_seed0_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "predictions_files = glob.glob(f\"{results_dir}/*.jsonl\")\n",
    "for pred_file in predictions_files:\n",
    "    print(pred_file)\n",
    "    # Here we get first pred_file. We will have more than one file if we run multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collect_results import read_jsonl_file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = read_jsonl_file(pred_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "Below we can print the prompts for the different choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Prompt for cand `entailment`:\n",
      "Y él dijo: Mamá, estoy en casa, ¿verdad? Sí, llamó a su madre tan pronto como el autobús escolar lo dejó.\n",
      "##########\n",
      "### Calibrations prompts for cand `entailment`:\n",
      "## calib option 0 prompt:\n",
      "Sí, llamó a su madre tan pronto como el autobús escolar lo dejó.\n",
      "### Prompt for cand `contradiction`:\n",
      "Y él dijo: Mamá, estoy en casa, ¿verdad? No, llamó a su madre tan pronto como el autobús escolar lo dejó.\n",
      "##########\n",
      "### Calibrations prompts for cand `contradiction`:\n",
      "## calib option 0 prompt:\n",
      "No, llamó a su madre tan pronto como el autobús escolar lo dejó.\n",
      "### Prompt for cand `neutral`:\n",
      "Y él dijo: Mamá, estoy en casa, ¿verdad? Además, llamó a su madre tan pronto como el autobús escolar lo dejó.\n",
      "##########\n",
      "### Calibrations prompts for cand `neutral`:\n",
      "## calib option 0 prompt:\n",
      "Además, llamó a su madre tan pronto como el autobús escolar lo dejó.\n"
     ]
    }
   ],
   "source": [
    "prediction = predictions[0]\n",
    "for cand, cand_info in prediction[\"candidates\"]:\n",
    "    print(f\"### Prompt for cand `{cand}`:\")\n",
    "    print(cand_info[\"meta\"][\"prompt\"])\n",
    "    \n",
    "    print(\"#\" * 10)\n",
    "    #print(cand_info[\"meta\"].keys())\n",
    "    if \"calib_metas\" in cand_info[\"meta\"]:\n",
    "        print(f\"### Calibrations prompts for cand `{cand}`:\")\n",
    "        for calib_id, calib_meta in enumerate(cand_info[\"meta\"][\"calib_metas\"]):\n",
    "            print(f\"## calib option {calib_id} prompt:\")\n",
    "            print(calib_meta[\"prompt\"])\n",
    "        \n",
    "    \n",
    "    #break  # print the first only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq-20210102",
   "language": "python",
   "name": "fairseq-20210102"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
