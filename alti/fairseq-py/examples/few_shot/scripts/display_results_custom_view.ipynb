{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read aggregated results \n",
    "The collect_results.py script aggregates results from multiple directories. You can view and manipulate the aggregated results to get insights from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the raw results file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the path to the raw results file generated with the script\n",
    "results_json = \"/private/home/tbmihaylov/fairseq-xlmg/gpt3_repro_cb_results.tsv.raw.jsonl\"\n",
    "results_json = \"/private/home/tbmihaylov/fairseq-xlmg/cb_our_models.tsv.raw.jsonl\"\n",
    "results_json = \"/checkpoint/tbmihaylov/few_shot/xnli_experimental/results.tsv.raw.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 items loaded\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "with open(results_json) as f_res:\n",
    "    for line in f_res:\n",
    "        line = line.strip()\n",
    "        item = json.loads(line)\n",
    "        if \"macro_F1::scores\" in item:\n",
    "            item[\"macro_F1::max\"] = max(item[\"macro_F1::scores\"])\n",
    "            item[\"macro_F1::min\"] = max(item[\"macro_F1::scores\"])\n",
    "        if \"accuracy::scores\" in item:\n",
    "            item[\"accuracy::max\"] = max(item[\"accuracy::scores\"])\n",
    "            item[\"accuracy::min\"] = max(item[\"accuracy::scores\"])\n",
    "        results.append(item)\n",
    "\n",
    "print(f\"{len(results)} items loaded\")\n",
    "#print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results has the followinf columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_name',\n",
       " 'task',\n",
       " 'language',\n",
       " 'template',\n",
       " 'nb_few_shot_samples',\n",
       " 'calibration_options',\n",
       " 'calibrator_name',\n",
       " 'train_set',\n",
       " 'valid_set',\n",
       " 'eval_set',\n",
       " 'train_lang',\n",
       " 'valid_lang',\n",
       " 'ppl_common_prefix::scores',\n",
       " 'ppl_common_prefix::mean',\n",
       " 'ppl_common_prefix::std',\n",
       " 'ppl_common_prefix::mean_confidence_interval',\n",
       " 'ppl_selected_candidate::scores',\n",
       " 'ppl_selected_candidate::mean',\n",
       " 'ppl_selected_candidate::std',\n",
       " 'ppl_selected_candidate::mean_confidence_interval',\n",
       " 'ppl_full_selected_candidate::scores',\n",
       " 'ppl_full_selected_candidate::mean',\n",
       " 'ppl_full_selected_candidate::std',\n",
       " 'ppl_full_selected_candidate::mean_confidence_interval',\n",
       " 'ppl_candidates_full_prompt__entailment::scores',\n",
       " 'ppl_candidates_full_prompt__entailment::mean',\n",
       " 'ppl_candidates_full_prompt__entailment::std',\n",
       " 'ppl_candidates_full_prompt__entailment::mean_confidence_interval',\n",
       " 'ppl_candidates_full_prompt__contradiction::scores',\n",
       " 'ppl_candidates_full_prompt__contradiction::mean',\n",
       " 'ppl_candidates_full_prompt__contradiction::std',\n",
       " 'ppl_candidates_full_prompt__contradiction::mean_confidence_interval',\n",
       " 'ppl_candidates_full_prompt__neutral::scores',\n",
       " 'ppl_candidates_full_prompt__neutral::mean',\n",
       " 'ppl_candidates_full_prompt__neutral::std',\n",
       " 'ppl_candidates_full_prompt__neutral::mean_confidence_interval',\n",
       " 'ppl_candidates_full_prompt::scores',\n",
       " 'ppl_candidates_full_prompt::mean',\n",
       " 'ppl_candidates_full_prompt::std',\n",
       " 'ppl_candidates_full_prompt::mean_confidence_interval',\n",
       " 'ppl_candidates::scores',\n",
       " 'ppl_candidates::mean',\n",
       " 'ppl_candidates::std',\n",
       " 'ppl_candidates::mean_confidence_interval',\n",
       " 'accuracy::scores',\n",
       " 'accuracy::mean',\n",
       " 'accuracy::std',\n",
       " 'accuracy::mean_confidence_interval',\n",
       " 'ppl_answer_correct_gold::scores',\n",
       " 'ppl_answer_correct_gold::mean',\n",
       " 'ppl_answer_correct_gold::std',\n",
       " 'ppl_answer_correct_gold::mean_confidence_interval',\n",
       " 'ppl_answer_incorrect_gold::scores',\n",
       " 'ppl_answer_incorrect_gold::mean',\n",
       " 'ppl_answer_incorrect_gold::std',\n",
       " 'ppl_answer_incorrect_gold::mean_confidence_interval',\n",
       " 'ppl_answer_incorrect_std_gold::scores',\n",
       " 'ppl_answer_incorrect_std_gold::mean',\n",
       " 'ppl_answer_incorrect_std_gold::std',\n",
       " 'ppl_answer_incorrect_std_gold::mean_confidence_interval',\n",
       " 'ppl_answer_incorrect_min_gold::scores',\n",
       " 'ppl_answer_incorrect_min_gold::mean',\n",
       " 'ppl_answer_incorrect_min_gold::std',\n",
       " 'ppl_answer_incorrect_min_gold::mean_confidence_interval',\n",
       " 'ppl_answer_correct_lt_incorrect_gold::scores',\n",
       " 'ppl_answer_correct_lt_incorrect_gold::mean',\n",
       " 'ppl_answer_correct_lt_incorrect_gold::std',\n",
       " 'ppl_answer_correct_lt_incorrect_gold::mean_confidence_interval',\n",
       " 'ppl_full_correct_gold::scores',\n",
       " 'ppl_full_correct_gold::mean',\n",
       " 'ppl_full_correct_gold::std',\n",
       " 'ppl_full_correct_gold::mean_confidence_interval',\n",
       " 'ppl_full_incorrect_gold::scores',\n",
       " 'ppl_full_incorrect_gold::mean',\n",
       " 'ppl_full_incorrect_gold::std',\n",
       " 'ppl_full_incorrect_gold::mean_confidence_interval',\n",
       " 'ppl_full_incorrect_std_gold::scores',\n",
       " 'ppl_full_incorrect_std_gold::mean',\n",
       " 'ppl_full_incorrect_std_gold::std',\n",
       " 'ppl_full_incorrect_std_gold::mean_confidence_interval',\n",
       " 'ppl_full_incorrect_min_gold::scores',\n",
       " 'ppl_full_incorrect_min_gold::mean',\n",
       " 'ppl_full_incorrect_min_gold::std',\n",
       " 'ppl_full_incorrect_min_gold::mean_confidence_interval',\n",
       " 'ppl_full_correct_lt_incorrect_gold::scores',\n",
       " 'ppl_full_correct_lt_incorrect_gold::mean',\n",
       " 'ppl_full_correct_lt_incorrect_gold::std',\n",
       " 'ppl_full_correct_lt_incorrect_gold::mean_confidence_interval',\n",
       " 'execution_time::scores',\n",
       " 'execution_time::mean',\n",
       " 'execution_time::std',\n",
       " 'execution_time::mean_confidence_interval',\n",
       " 'run_params::scoring',\n",
       " 'run_params::uniform_sampling',\n",
       " 'run_params::predictor_name',\n",
       " 'run_params::max_positions',\n",
       " 'run_params::train_sep',\n",
       " 'run_params::xnli_template',\n",
       " 'run_params::predictions_dump_dir',\n",
       " 'run_params::results_dir',\n",
       " 'run_params::add_prompt_to_meta',\n",
       " 'run_params::add_positional_scores_to_meta',\n",
       " 'run_params::add_prompt_tokens_to_meta',\n",
       " 'run_params::add_calib_meta',\n",
       " 'run_params::model_name_display',\n",
       " 'results_file',\n",
       " 'results_file_ctime',\n",
       " 'results_file_mtime',\n",
       " 'calibration',\n",
       " 'checkpoint_steps',\n",
       " 'eval_examples_cnt::scores',\n",
       " 'eval_examples_cnt::mean',\n",
       " 'eval_examples_cnt::std',\n",
       " 'eval_examples_cnt::mean_confidence_interval',\n",
       " 'accuracy__raw::scores',\n",
       " 'accuracy__raw::mean',\n",
       " 'accuracy__raw::std',\n",
       " 'accuracy__raw::mean_confidence_interval',\n",
       " 'accuracy_run::scores',\n",
       " 'accuracy_run::mean',\n",
       " 'accuracy_run::std',\n",
       " 'accuracy_run::mean_confidence_interval',\n",
       " '_metric',\n",
       " '_metric_val',\n",
       " '_metric_val_std',\n",
       " 'accuracy::max',\n",
       " 'accuracy::min']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a custom view using pandas\n",
    "You can use the pandas api to manipulate the table and create custom views. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'macro_F1::mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5ec445c66fde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_params::train_sep\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"macro_F1::mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"macro_F1::std\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_json\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_macroF1.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-5ec445c66fde>\u001b[0m in \u001b[0;36mcb_view\u001b[0;34m(df, values)\u001b[0m\n\u001b[1;32m      9\u001b[0m     pt = pd.pivot_table(df, values=values, index=[\"task\", \"language\", \"template\", \"nb_few_shot_samples\", \"_metric\", \"calibration\", \"run_params::scoring\", \"run_params::train_sep\"],\n\u001b[1;32m     10\u001b[0m                         columns=['model_name',\n\u001b[0;32m---> 11\u001b[0;31m                                 ], aggfunc=np.mean)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswaplevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/fairseq/lib/python3.7/site-packages/pandas/core/reshape/pivot.py\u001b[0m in \u001b[0;36mpivot_table\u001b[0;34m(data, values, index, columns, aggfunc, fill_value, margins, dropna, margins_name, observed)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mto_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'macro_F1::mean'"
     ]
    }
   ],
   "source": [
    "def my_custom_view(df):\n",
    "    pt = pd.pivot_table(df, values=[\"_metric_val\", \"_metric_val_std\"], index=[\"task\", \"language\", \"template\", \"nb_few_shot_samples\", \"_metric\", \"calibration\", \"run_params::scoring\", \"run_params::train_sep\"],\n",
    "                        columns=['model_name',\n",
    "                                ], aggfunc=np.mean)\n",
    "    pt = pt.swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "    return pt\n",
    "\n",
    "def cb_view(df, values=[\"_metric_val\", \"_metric_val_std\"]):\n",
    "    pt = pd.pivot_table(df, values=values, index=[\"task\", \"language\", \"template\", \"nb_few_shot_samples\", \"_metric\", \"calibration\", \"run_params::scoring\", \"run_params::train_sep\"],\n",
    "                        columns=['model_name',\n",
    "                                ], aggfunc=np.mean)\n",
    "    pt = pt.swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "    return pt\n",
    "\n",
    "filtered_df = df[df[\"run_params::train_sep\"] != None]\n",
    "\n",
    "pt = cb_view(filtered_df, values=[\"macro_F1::mean\", \"macro_F1::std\"])\n",
    "pt.to_csv(results_json+\"_macroF1.tsv\", sep=\"\\t\")\n",
    "\n",
    "pt = cb_view(filtered_df, values=[\"accuracy::mean\", \"accuracy::std\"])\n",
    "pt.to_csv(results_json+\"_acc.tsv\", sep=\"\\t\")\n",
    "\n",
    "pt = cb_view(filtered_df, values=[\"micro_F1::mean\", \"micro_F1::std\"])\n",
    "pt.to_csv(results_json+\"_microF1.tsv\", sep=\"\\t\")\n",
    "\n",
    "pt = cb_view(filtered_df, values=[\"macro_F1::max\", \"accuracy::max\"])\n",
    "out_file = results_json+\"_max.tsv\"\n",
    "pt.to_csv(out_file, sep=\"\\t\")\n",
    "print(f\"exported to {out_file}\")\n",
    "\n",
    "pt = cb_view(filtered_df, values=[\"micro_F1::mean\", \"micro_F1::std\", \"macro_F1::max\", \"accuracy::mean\", \"accuracy::std\", \"accuracy::max\"])\n",
    "out_file = results_json+\"_all.tsv\"\n",
    "pt.to_csv(out_file, sep=\"\\t\")\n",
    "print(f\"exported to {out_file}\")\n",
    "\n",
    "\n",
    "pt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make your view available to others\n",
    "If you think that you created a view that might be useful to others, you can add it to the collect_results.py.\n",
    "Simply add the `my_custom_view` function to the `display_views` dictionary in [collect_results.py](examples/few_shot/scripts/collect_results.py) and the custom view will be available as -v my_custom_view_freindly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df[\"run_params::train_sep\"] == \"\\n\\n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq",
   "language": "python",
   "name": "fairseq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
