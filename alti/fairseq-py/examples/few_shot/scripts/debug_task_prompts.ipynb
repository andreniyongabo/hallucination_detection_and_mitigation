{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging task prompts\n",
    "This notebook can be used to debug task prompts.\n",
    "To do that we want to modify the template (the example is with copa) by creating a new template class (e.g. COPAGPT3Template) at examples/few_shot/templates.py usually by inheriting the default class (COPATemplate). We do not want to update the template in place since we might have experiments run with the existing templates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os    \n",
    "import glob\n",
    "from pathlib import Path\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to update the parameters below to correspond to our new defined class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"125M_gpt3_setting\"\n",
    "predictor_name=\"clmprompting\"\n",
    "# model_name = \"openai_ada\"\n",
    "# predictor_name=\"CLMPromptingOpenaiApi\"\n",
    "\n",
    "scoring = \"sum\"\n",
    "\n",
    "#debug_task=\"cb\"\n",
    "#template_name=\"cb_gpt3_reproduce\"\n",
    "\n",
    "debug_task=\"xnli\"\n",
    "template_name=\"xnli_mt__bg\"\n",
    "calibrator_name = None #\"average_option\"\n",
    "calibration_options = [] #[\"sentence1::|sentence2::\"]\n",
    "languages=[\"en\"]\n",
    "train_lang=\"de\"\n",
    "\n",
    "current_user = os.getenv(\"USER\")\n",
    "time_stamp = datetime.datetime.today().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "results_dir = f\"/checkpoint/{current_user}/few_shot/debug_prompts/pred_{debug_task}_{template_name}_{model_name}_{time_stamp}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run experiment \n",
    "We want to run experiment with some small parameters since the prompt generation for few-shot learning usually depends on some logic in the predictor class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name=125M_gpt3_setting\n",
      "model_pretraining_valid_info={\"epoch\": 3, \"valid_loss\": \"3.926\", \"valid_ppl\": \"15.2\", \"valid_wps\": \"857595\", \"valid_wpb\": \"216347\", \"valid_bsz\": \"105.7\", \"valid_num_updates\": \"572204\", \"valid_best_loss\": \"3.926\", \"time_stamp\": \"2020-11-04 04:05:02\", \"log_file\": \"/large_experiments/xlmg/models/dense/125M/few_shot.roberta+cc100.os.bm_none.tps2048.transformer_lm_gpt.share.adam.b2_0.98.eps1e-08.cl0.0.lr0.005.wu715.dr0.1.atdr0.1.wd0.01.ms4.uf2.mu572204.s1.ngpu32/train.log\"}\n",
      "distributed_training.distributed_port=-1\n",
      "Loaded model\n",
      "model_loading_time=4.0 seconds\n",
      "model_loading_time_cuda=5.6 seconds\n",
      "Changing max_positions from 2048 to 1024\n",
      "task=xnli\n",
      "eval_set=dev\n",
      "eval language=en\n",
      "train_set=test\n",
      "train_lang=de\n",
      "template=xnli_mt__bg\n",
      "calibration_options=[]\n",
      "nb_few_shot_samples=1\n",
      "expected_max_tgt_len=193, max_positions=1024\n",
      "Average number of train samples: 1.00\n",
      "Predicting 3 samples with 9 prompts..\n",
      "Before running model, bs=1, max_tgt_len=193 mem=0.24GB\n",
      "Predictions dumped to /checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_mt__bg_125M_gpt3_setting_2021-08-05-21-19-22/task.xnli_tmp.xnli_mt__bg_train.test.de_val.None.None_eval.dev.en_calib.None_fs1_seed0_predictions.jsonl\n",
      "results={'model_name': '125M_gpt3_setting', 'task': 'xnli', 'language': 'en', 'template': 'xnli_mt__bg', 'nb_few_shot_samples': 1, 'calibration_options': [], 'calibrator_name': None, 'train_set': 'test', 'valid_set': None, 'eval_set': 'dev', 'train_lang': 'de', 'valid_lang': None, 'ppl_common_prefix': {'scores': [10.73302427927653], 'mean': 10.73302427927653, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_selected_candidate': {'scores': [1.0640958944956462], 'mean': 1.0640958944956462, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_selected_candidate': {'scores': [9.824786504109701], 'mean': 9.824786504109701, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt__entailment': {'scores': [10.387644449869791], 'mean': 10.387644449869791, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt__contradiction': {'scores': [9.824786504109701], 'mean': 9.824786504109701, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt__neutral': {'scores': [9.955156326293945], 'mean': 9.955156326293945, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates_full_prompt': {'scores': [10.055862426757812], 'mean': 10.055862426757812, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_candidates': {'scores': [2.4453558127085366], 'mean': 2.4453558127085366, 'std': 0.0, 'mean_confidence_interval': nan}, 'accuracy': {'scores': [33.333333333333336], 'mean': 33.333333333333336, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_correct_gold': {'scores': [2.443040450414022], 'mean': 2.443040450414022, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_incorrect_gold': {'scores': [2.4465134938557944], 'mean': 2.4465134938557944, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_incorrect_std_gold': {'scores': [0.7517619927724203], 'mean': 0.7517619927724203, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_incorrect_min_gold': {'scores': [1.694751501083374], 'mean': 1.694751501083374, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_answer_correct_lt_incorrect_gold': {'scores': [33.333333333333336], 'mean': 33.333333333333336, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_correct_gold': {'scores': [10.060051282246908], 'mean': 10.060051282246908, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_incorrect_gold': {'scores': [10.053767999013266], 'mean': 10.053767999013266, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_incorrect_std_gold': {'scores': [0.18616565068562826], 'mean': 0.18616565068562826, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_incorrect_min_gold': {'scores': [9.867602348327637], 'mean': 9.867602348327637, 'std': 0.0, 'mean_confidence_interval': nan}, 'ppl_full_correct_lt_incorrect_gold': {'scores': [33.333333333333336], 'mean': 33.333333333333336, 'std': 0.0, 'mean_confidence_interval': nan}, 'execution_time': {'scores': [3.3651100285351276], 'mean': 3.3651100285351276, 'std': 0.0, 'mean_confidence_interval': nan}}\n",
      "\n",
      "ppl_selected_candidate                    = 1.0641\n",
      "ppl_full_selected_candidate               = 9.8248\n",
      "ppl_full_incorrect_std_gold               = 0.1862\n",
      "ppl_full_incorrect_min_gold               = 9.8676\n",
      "ppl_full_incorrect_gold                   = 10.0538\n",
      "ppl_full_correct_lt_incorrect_gold        = 33.3333\n",
      "ppl_full_correct_gold                     = 10.0601\n",
      "ppl_common_prefix                         = 10.733\n",
      "ppl_candidates_full_prompt__neutral       = 9.9552\n",
      "ppl_candidates_full_prompt__entailment    = 10.3876\n",
      "ppl_candidates_full_prompt__contradiction = 9.8248\n",
      "ppl_candidates_full_prompt                = 10.0559\n",
      "ppl_candidates                            = 2.4454\n",
      "ppl_answer_incorrect_std_gold             = 0.7518\n",
      "ppl_answer_incorrect_min_gold             = 1.6948\n",
      "ppl_answer_incorrect_gold                 = 2.4465\n",
      "ppl_answer_correct_lt_incorrect_gold      = 33.3333\n",
      "ppl_answer_correct_gold                   = 2.443\n",
      "execution_time                            = 3.3651\n",
      "accuracy                                  = 33.3333\n"
     ]
    }
   ],
   "source": [
    "from examples.few_shot.gpt3_eval import run_evaluations_from_model_name\n",
    "\n",
    "args = {\n",
    "    \"model_name\": model_name,\n",
    "    \"tasks\": [debug_task],\n",
    "    f\"{debug_task}_template\": template_name,\n",
    "    \"nb_few_shot_samples_values\": [1],\n",
    "    \"n_eval_samples\": 3, # set 1 for debug\n",
    "    \"num_trials\": 1,\n",
    "    \"max_positions\": 1024,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"scoring\": scoring,\n",
    "    \"calibrator_name\": calibrator_name,\n",
    "    f\"{debug_task}_calibration_options\": calibration_options,\n",
    "    f\"{debug_task}_languages\": languages,\n",
    "    \"predictions_dump_dir\": results_dir,\n",
    "    \"results_dir\": results_dir,\n",
    "    \"add_prompt_to_meta\": True,\n",
    "    \"add_positional_scores_to_meta\": True, \n",
    "    \"add_prompt_tokens_to_meta\": True,\n",
    "    \"add_calib_meta\": True,\n",
    "    \"train_sep\": \"\\n\",\n",
    "    \"xnli_train_lang\": train_lang,\n",
    "    #field_sep=\"\\n\",\n",
    "    \"uniform_sampling\": False,\n",
    "    \"predictor_name\": predictor_name, #\"CLMPromptingOpenaiApi\",\n",
    "}\n",
    "results = run_evaluations_from_model_name(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read predictions file\n",
    "We will read the predictions file which has the generated prompt, token ids and scores tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_name': '125M_gpt3_setting',\n",
       "  'task': 'xnli',\n",
       "  'language': 'en',\n",
       "  'template': 'xnli_mt__bg',\n",
       "  'nb_few_shot_samples': 1,\n",
       "  'calibration_options': [],\n",
       "  'calibrator_name': None,\n",
       "  'train_set': 'test',\n",
       "  'valid_set': None,\n",
       "  'eval_set': 'dev',\n",
       "  'train_lang': 'de',\n",
       "  'valid_lang': None,\n",
       "  'ppl_common_prefix': {'scores': [10.73302427927653],\n",
       "   'mean': 10.73302427927653,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_selected_candidate': {'scores': [1.0640958944956462],\n",
       "   'mean': 1.0640958944956462,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_selected_candidate': {'scores': [9.824786504109701],\n",
       "   'mean': 9.824786504109701,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt__entailment': {'scores': [10.387644449869791],\n",
       "   'mean': 10.387644449869791,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt__contradiction': {'scores': [9.824786504109701],\n",
       "   'mean': 9.824786504109701,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt__neutral': {'scores': [9.955156326293945],\n",
       "   'mean': 9.955156326293945,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates_full_prompt': {'scores': [10.055862426757812],\n",
       "   'mean': 10.055862426757812,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_candidates': {'scores': [2.4453558127085366],\n",
       "   'mean': 2.4453558127085366,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'accuracy': {'scores': [33.333333333333336],\n",
       "   'mean': 33.333333333333336,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_correct_gold': {'scores': [2.443040450414022],\n",
       "   'mean': 2.443040450414022,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_incorrect_gold': {'scores': [2.4465134938557944],\n",
       "   'mean': 2.4465134938557944,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_incorrect_std_gold': {'scores': [0.7517619927724203],\n",
       "   'mean': 0.7517619927724203,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_incorrect_min_gold': {'scores': [1.694751501083374],\n",
       "   'mean': 1.694751501083374,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_answer_correct_lt_incorrect_gold': {'scores': [33.333333333333336],\n",
       "   'mean': 33.333333333333336,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_correct_gold': {'scores': [10.060051282246908],\n",
       "   'mean': 10.060051282246908,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_incorrect_gold': {'scores': [10.053767999013266],\n",
       "   'mean': 10.053767999013266,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_incorrect_std_gold': {'scores': [0.18616565068562826],\n",
       "   'mean': 0.18616565068562826,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_incorrect_min_gold': {'scores': [9.867602348327637],\n",
       "   'mean': 9.867602348327637,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'ppl_full_correct_lt_incorrect_gold': {'scores': [33.333333333333336],\n",
       "   'mean': 33.333333333333336,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'execution_time': {'scores': [3.3651100285351276],\n",
       "   'mean': 3.3651100285351276,\n",
       "   'std': 0.0,\n",
       "   'mean_confidence_interval': nan},\n",
       "  'run_params': {'xnli_template': 'xnli_mt__bg',\n",
       "   'n_eval_samples': 3,\n",
       "   'max_positions': 1024,\n",
       "   'max_tokens': 1024,\n",
       "   'scoring': 'sum',\n",
       "   'calibrator_name': None,\n",
       "   'xnli_calibration_options': [],\n",
       "   'xnli_languages': ['en'],\n",
       "   'predictions_dump_dir': '/checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_mt__bg_125M_gpt3_setting_2021-08-05-21-19-22',\n",
       "   'results_dir': '/checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_mt__bg_125M_gpt3_setting_2021-08-05-21-19-22',\n",
       "   'add_prompt_to_meta': True,\n",
       "   'add_positional_scores_to_meta': True,\n",
       "   'add_prompt_tokens_to_meta': True,\n",
       "   'add_calib_meta': True,\n",
       "   'train_sep': '\\n',\n",
       "   'xnli_train_lang': 'de',\n",
       "   'uniform_sampling': False,\n",
       "   'predictor_name': 'clmprompting',\n",
       "   'model_name_display': '125M_gpt3_setting'}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/checkpoint/tbmihaylov/few_shot/debug_prompts/pred_xnli_xnli_mt__bg_125M_gpt3_setting_2021-08-05-21-19-22/task.xnli_tmp.xnli_mt__bg_train.test.de_val.None.None_eval.dev.en_calib.None_fs1_seed0_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "predictions_files = glob.glob(f\"{results_dir}/*.jsonl\")\n",
    "for pred_file in predictions_files:\n",
    "    print(pred_file)\n",
    "    # Here we get first pred_file. We will have more than one file if we run multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collect_results import read_jsonl_file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = read_jsonl_file(pred_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "Below we can print the prompts for the different choices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Prompt for cand `entailment`:\n",
      "Параграф: The folks at L'academie Internationale des Arts et des Sciences Numeriques have innovated a clever variant on this trick. Въпрос: The people at the school just followed their lead. Вярно, грешно или нито едното? Грешно\n",
      "Параграф: And he said, Mama, I'm home. Въпрос: He called his mom as soon as the school bus dropped him off. Вярно, грешно или нито едното? Вярно\n",
      "##########\n"
     ]
    }
   ],
   "source": [
    "prediction = predictions[0]\n",
    "for cand, cand_info in prediction[\"candidates\"]:\n",
    "    print(f\"### Prompt for cand `{cand}`:\")\n",
    "    print(cand_info[\"meta\"][\"prompt\"])\n",
    "    \n",
    "    print(\"#\" * 10)\n",
    "    #print(cand_info[\"meta\"].keys())\n",
    "    if \"calib_metas\" in cand_info[\"meta\"]:\n",
    "        print(f\"### Calibrations prompts for cand `{cand}`:\")\n",
    "        for calib_id, calib_meta in enumerate(cand_info[\"meta\"][\"calib_metas\"]):\n",
    "            print(f\"## calib option {calib_id} prompt:\")\n",
    "            print(calib_meta[\"prompt\"])\n",
    "        \n",
    "    \n",
    "    break  # print the first only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fairseq-20210102",
   "language": "python",
   "name": "fairseq-20210102"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
